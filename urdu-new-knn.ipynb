{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9865305,"sourceType":"datasetVersion","datasetId":6055323}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"anaconda-cloud":{},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- KNN --- #","metadata":{"id":"NSz29akHKMyb","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This code imports a mix of libraries needed for handling files, processing audio, and building machine learning models. It uses `librosa` for audio feature extraction, `sklearn` for training and evaluating models, and `matplotlib`/`seaborn` for creating visualizations. It also suppresses warnings related to audio signal processing to keep the output clean while running the code.","metadata":{}},{"cell_type":"code","source":"import os\nimport warnings\nimport glob\nimport librosa\nimport librosa.display\nimport numpy as np\nimport pickle\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datasets import Dataset\n\nfrom sklearn import svm, model_selection\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import learning_curve\n\n# Suppress specific warning about n_fft size from librosa\nwarnings.filterwarnings(\"ignore\", message=\"n_fft=1024 is too large for input signal\")\nwarnings.filterwarnings(\"ignore\", message=\"n_fft=2048 is too large for input signal\")\n\n# Libraries like os, glob, and librosa are imported for file handling, audio processing, \n# and visualization. sklearn and pickle are used for model training, evaluation, and saving.","metadata":{"id":"jclOLxynKMyc","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This code sets up the directories for loading and saving data, specifically targeting the `normalized` folder for input and the `working` directory for output. It then counts and sums the number of `.wav` files in each emotion category (Anger, Happiness, Sadness, Neutral) within the training data, and prints the total number of audio files across all categories.","metadata":{}},{"cell_type":"code","source":"# Loading audio files\n\n# input_dir = '/kaggle/input'\n# for root, dirs, files in os.walk(input_dir):\n#     for dir_name in dirs:\n#         print(os.path.join(root, dir_name))\n\ninput_dir = '/kaggle/input/normalized'\noutput_dir = \"/kaggle/working\"\n\n# Emotions\ncategories = [\"Anger\", \"Happiness\", \"Sadness\", \"Neutral\"]\n\n# Count files in each category folder and get the combined total\ntrain_total = 0\nfor category in categories:\n    category_path = os.path.join(input_dir, category)\n    audio_files = [f for f in os.listdir(category_path) if f.endswith('.wav')]\n    train_total += len(audio_files)\n\nprint(f\"Total number of files in training data (combined across '{', '.join(categories)}' folders): {train_total}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This code defines a function to extract audio features from `.wav` files using `librosa`, including MFCCs, chroma, mel spectrogram, spectral contrast, and tonnetz. It processes audio files from different emotion categories, extracts the features, and splits the dataset into training, validation, and test sets using Hugging Face's `Dataset` format. Finally, it prints the sizes of each dataset split.","metadata":{}},{"cell_type":"code","source":"# Function to extract features from an audio file\ndef extract_feature(file_name):\n    try:\n        # Load the audio file using librosa\n        X, sample_rate = librosa.load(file_name)\n        \n        # Set a consistent, smaller n_fft value for short signals\n        n_fft = 512\n\n        # Pad the audio file to at least n_fft length if shorter\n        if len(X) < n_fft:\n            X = np.pad(X, (0, n_fft - len(X)))\n\n        # Calculate the Short-Time Fourier Transform (STFT)\n        stft = np.abs(librosa.stft(X, n_fft=n_fft))\n\n        # Extract Mel-frequency cepstral coefficients (MFCCs)\n        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n\n        # Extract chroma feature (pitch class)\n        chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n\n        # Extract Mel spectrogram\n        mel = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T, axis=0)\n\n        # Extract spectral contrast\n        contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T, axis=0)\n\n        # Extract tonnetz (harmonic features)\n        tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T, axis=0)\n        \n        # Return all features concatenated into a single vector\n        return np.hstack([mfccs, chroma, mel, contrast, tonnetz])\n\n    except Exception as e:\n        # Handle errors in feature extraction\n        print(f\"Error extracting features from {file_name}: {e}\")\n        return None\n\n# Function to parse all audio files from each category and extract features\ndef parse_audio_files(base_dir, categories):\n    features, labels = [], []\n    \n    # Loop through each category folder\n    for category in categories:\n        category_path = os.path.join(base_dir, category)\n        \n        # Get all .wav files in the current category\n        audio_files = glob.glob(os.path.join(category_path, '*.wav'))\n        \n        # Process each audio file\n        for fn in audio_files:\n            ext_features = extract_feature(fn)\n            \n            # If feature extraction is successful, append features and label\n            if ext_features is not None:\n                features.append(ext_features)\n                labels.append(category)\n    \n    # Convert features and labels to numpy arrays\n    return np.array(features), np.array(labels)\n\n# Parse the training data with all files per category\ntr_features, tr_labels = parse_audio_files(input_dir, categories)\n\n# Convert dataset to Hugging Face format\ndataset = Dataset.from_dict({\n    \"audio\": tr_features.tolist(),\n    \"label\": tr_labels.tolist()\n})\n\n# Split dataset into training, validation, and test sets\ntrain_val_dataset = dataset.train_test_split(test_size=0.2, seed=42)\ntrain_dataset, val_dataset = train_val_dataset['train'].train_test_split(test_size=0.1, seed=42).values()\ntest_dataset = train_val_dataset['test']\n\n# Output dataset sizes\nprint(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(val_dataset)}\")\nprint(f\"Test dataset size: {len(test_dataset)}\")","metadata":{"scrolled":true,"id":"HB0CNIK7KMyd","outputId":"c9b5c42c-93ea-4cd3-ed56-5dfd41f8779a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This code converts the audio features and labels from the training, validation, and test datasets into numpy arrays. It then trains a K-Nearest Neighbors (KNN) classifier using the training data, prints a success message, and saves the trained model to a file using `pickle` for later use.","metadata":{}},{"cell_type":"code","source":"# Convert Dataset objects to numpy arrays\nX_train = np.array(train_dataset['audio'], dtype=np.float32)\ny_train = np.array(train_dataset['label'], dtype=str)\nX_val = np.array(val_dataset['audio'], dtype=np.float32)\ny_val = np.array(val_dataset['label'], dtype=str)\nX_test = np.array(test_dataset['audio'], dtype=np.float32)\ny_test = np.array(test_dataset['label'], dtype=str)\n\n# Train the KNN model\nneigh = KNeighborsClassifier(n_neighbors=5)\nneigh.fit(X_train, y_train)\nprint(\"Model trained successfully on the training dataset.\")\n\n# Save the model\nfilename = 'Ensemble_Model_protocol_knn.sav'\npickle.dump(neigh, open(filename, 'wb'))\nprint('Model Saved.')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This code extracts features from `.wav` audio files using `librosa`, including MFCCs, chroma, mel spectrogram, spectral contrast, and tonnetz. It processes all audio files in given categories, combines the features into a single vector for each file, and stores the features along with their respective labels. Afterward, it checks if the test dataset is properly formatted, uses the trained model to make predictions on the test data, and then evaluates the modelâ€™s performance by displaying the accuracy score and confusion matrix.","metadata":{"id":"Zc-HCOmkKMyf"}},{"cell_type":"code","source":"# List to store file names\ntarget_files = []\n\n# Method to extract features from speech using librosa\ndef extract_feature(file_name):\n    try:\n        # Load the audio file using librosa\n        X, sample_rate = librosa.load(file_name)\n        \n        # Pad short audio files to ensure they have a minimum length of 512\n        if len(X) < 512:\n            X = np.pad(X, (0, 512 - len(X)))\n        \n        # Compute the Short-Time Fourier Transform (STFT)\n        stft = np.abs(librosa.stft(X, n_fft=512))\n        \n        # Extract Mel-frequency cepstral coefficients (MFCCs)\n        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n        \n        # Extract chroma feature (pitch class)\n        chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n        \n        # Extract Mel spectrogram\n        mel = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T, axis=0)\n        \n        # Extract spectral contrast\n        contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T, axis=0)\n        \n        # Extract tonnetz (harmonic features)\n        tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T, axis=0)\n        \n        # Return all extracted features\n        return mfccs, chroma, mel, contrast, tonnetz\n\n    except Exception as e:\n        # Handle errors during feature extraction\n        print(f\"Error extracting features from {file_name}: {e}\")\n        return None  # Return None if extraction fails\n\n# Updated method to parse all audio files from each category folder\ndef parse_audio_files(base_dir, categories):\n    features, labels = [], []\n    \n    # Loop through each category folder\n    for category in categories:\n        category_path = os.path.join(base_dir, category)\n        \n        # Load all .wav files in the current category folder\n        audio_files = glob.glob(os.path.join(category_path, '*.wav'))\n        \n        # Process each audio file\n        for fn in audio_files:\n            ext_features = extract_feature(fn)\n            \n            # If feature extraction is successful, append the features and label\n            if ext_features is not None:\n                mfccs, chroma, mel, contrast, tonnetz = ext_features\n                \n                # Combine all features into a single vector\n                combined_features = np.hstack([mfccs, chroma, mel, contrast, tonnetz])\n                \n                # Append the feature and its corresponding label\n                features.append(combined_features)\n                labels.append(category)  # Use the folder name as the label\n                \n                # Store the file name for later use\n                target_files.append(fn)\n                \n    # Convert features and labels to numpy arrays\n    return np.array(features), np.array(labels)\n\n# Ensure test_dataset is in the proper format\nif \"audio\" in test_dataset.column_names and \"label\" in test_dataset.column_names:\n    # Convert test dataset's \"audio\" and \"label\" columns to numpy arrays\n    ts_features = np.array(test_dataset[\"audio\"], dtype=np.float32)\n    ts_labels = np.array(test_dataset[\"label\"], dtype=str)\n\n    # Predict using the trained model\n    prediction = neigh.predict(ts_features)\n\n    # Create and plot confusion matrix\n    matrix = confusion_matrix(ts_labels, prediction)\n    \n    # Get unique classes from the test labels and predictions\n    unique_classes = sorted(list(set(ts_labels).union(set(prediction))))\n\n    # Create a DataFrame for the confusion matrix\n    df = pd.DataFrame(matrix, columns=unique_classes, index=unique_classes)\n\n    # Display evaluation metrics\n    accuracy = accuracy_score(ts_labels, prediction)\n    correct_predictions = accuracy_score(ts_labels, prediction, normalize=False)\n    print(f'Accuracy Score: {accuracy}')\n    print(f'Number of Correct Predictions: {correct_predictions} out of {len(ts_labels)}')\nelse:\n    print(\"test_dataset must contain 'audio' and 'label' fields. Please verify the dataset format.\")\n","metadata":{"id":"jmE0C4UjKMyf","outputId":"6054c924-cc27-401f-c36f-2d460d7027ec","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This code evaluates the trained KNN model's performance on the training, validation, and test datasets. It computes accuracy scores for each dataset and generates a confusion matrix heatmap for the test set to visualize classification performance. Additionally, the code plots accuracy trends for training and validation data over time, and uses `learning_curve` from `sklearn` to calculate and plot the learning curve with cross-validation, displaying the model's performance as more training data is used. The learning curve shows the relationship between training size and model accuracy, with standard deviation bands highlighting the variability.","metadata":{}},{"cell_type":"code","source":"# Evaluate on training, validation, and test data\ntrain_accuracy = accuracy_score(y_train, neigh.predict(X_train))\nval_accuracy = accuracy_score(y_val, neigh.predict(X_val))\ntest_accuracy = accuracy_score(y_test, neigh.predict(X_test))\n\n# Calculate confusion matrix for test data\nprediction = neigh.predict(X_test)\nconf_matrix = confusion_matrix(y_test, prediction)\n\n# Plot confusion matrix heatmap\ncategory_names = sorted(list(set(y_val).union(set(prediction))))  # Unique category names\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=category_names, yticklabels=category_names)\nplt.title('Confusion Matrix Heatmap')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()\n\n# Print final evaluation metrics\nprint(f\"Train Accuracy: {train_accuracy:.4f}\")\nprint(f\"Validation Accuracy: {val_accuracy:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\n\n# Plot Accuracy (Single plot)\nplt.figure(figsize=(12, 5))\n\n# Accuracy plot (Training vs Validation vs Test) with labels\nplt.plot([train_accuracy], label='Training Accuracy', marker='o', markersize=8, color='blue')\nplt.plot([val_accuracy], label='Validation Accuracy', marker='o', markersize=8, color='red')\n# plt.plot([test_accuracy], label='Test Accuracy', marker='o', markersize=8, color='blue')\n\n# Customize the plot\nplt.xticks([0, 1, 2])  # Set x-axis ticks for clarity\nplt.xlabel('Accuracy Type')  # Label the x-axis\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Training and Validation Accuracy')\nplt.grid(True)  # Add a grid for better readability\n\nplt.tight_layout()\nplt.show()\n\nfrom sklearn.model_selection import KFold, learning_curve\nimport matplotlib.pyplot as plt\n\n# Define the KFold object for cross-validation\nseed = 42\nkfold = KFold(n_splits=5, shuffle=True, random_state=seed)\n\n# Use learning_curve to calculate scores\ntrain_sizes, train_scores, test_scores = learning_curve(\n    neigh, X_train, y_train, n_jobs=-1, cv=kfold, train_sizes=np.linspace(.1, 1.0, 5), verbose=1\n)\n\n# Calculate mean and standard deviation of scores\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\n\n# Plot the learning curve\nplt.figure(figsize=(8, 4))\nplt.title(\"KNN Model Learning Curve\")\nplt.xlabel(\"Training examples\")\nplt.ylabel(\"Score\")\nplt.grid(True)\n\n# Plot the mean scores with standard deviation bands\nplt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\nplt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\nplt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\nplt.plot(train_sizes, test_scores_mean, 'o-', color=\"b\", label=\"Cross-validation score\")\n\n# Plot the test score as a separate line\n# plt.plot(train_sizes, [test_accuracy] * len(train_sizes), 'o-', color='b', label='Test score')\n\n# Set the y-axis limits\nplt.ylim(0, 1.1)\n\n# Add legend and show the plot\nplt.legend(loc=\"best\")\nplt.show()","metadata":{"id":"s8rn_peBKMyg","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(ts_labels, prediction, target_names=category_names))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for overlaps between train and test datasets\ntrain_set = set(map(tuple, tr_features))\ntest_set = set(map(tuple, ts_features))\noverlap = train_set.intersection(test_set)\nprint(f\"Number of overlapping samples: {len(overlap)}\")\nprint(conf_matrix)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_accuracy = neigh.score(tr_features, tr_labels)\ntest_accuracy = accuracy_score(ts_labels, prediction)\nprint(f\"Training Accuracy: {train_accuracy:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\ncv_scores = cross_val_score(neigh, tr_features, tr_labels, cv=5)\nprint(f\"Cross-Validation Scores: {cv_scores}\")\nprint(f\"Mean Cross-Validation Accuracy: {np.mean(cv_scores):.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"misclassified_indices = np.where(ts_labels != prediction)[0]\nprint(\"Misclassified Samples:\", misclassified_indices[:10])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}